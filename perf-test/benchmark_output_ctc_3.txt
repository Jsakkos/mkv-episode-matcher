2025-12-25 22:55:44.497 | INFO     | mkv_episode_matcher.config:<module>:19 - Total available threads: 12 -> Setting max to 4
2025-12-25 22:55:44.500 | INFO     | mkv_episode_matcher.__main__:<module>:18 - Starting the application
+-----------------------------------------------+
| Simplified GPU Benchmark: Whisper vs Parakeet |
+-----------------------------------------------+
CUDA Device: NVIDIA GeForce RTX 3090

Test File: The Expanse - S01E01.mkv
Detected Show Name: The Expanse
Target: S01E01
Expected Cache Path: C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The 
Expanse
Cache directory exists.
Found 122 SRT files.

Testing parakeet:nvidia/parakeet-ctc-0.6b...
Identifying The Expanse - S01E01.mkv (S1)...
Found 10 reference files.
[NeMo W 2025-12-25 22:56:01 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.
W1225 22:56:01.273000 37820 Lib\site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.
No exporters were provided. This means that no telemetry data will be collected.
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[NeMo I 2025-12-25 22:56:35 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens
[NeMo W 2025-12-25 22:56:35 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-train-all.json
    sample_rate: 16000
    batch_size: 16
    shuffle: true
    num_workers: 8
    pin_memory: true
    use_start_end_token: false
    trim_silence: false
    max_duration: 16.7
    min_duration: 0.1
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    bucketing_strategy: fully_randomized
    bucketing_batch_size: null
    
[NeMo W 2025-12-25 22:56:35 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-dev-clean.json
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo W 2025-12-25 22:56:35 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
    Test config : 
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo I 2025-12-25 22:56:35 nemo_logging:393] PADDING: 0
[NeMo I 2025-12-25 22:56:42 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from C:\Users\Jonathan\.cache\huggingface\hub\models--nvidia--parakeet-ctc-0.6b\snapshots\ad09ba1cc62743fbc9814de5d2016fca9096485a\parakeet-ctc-0.6b.nemo.
Processing chunk at 300s...
[NeMo W 2025-12-25 22:56:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:56:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s][NeMo W 2025-12-25 22:56:47 nemo_logging:405] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.

Transcribing: 1it [00:03,  3.15s/it]
Transcribing: 1it [00:03,  3.15s/it]
Transcribed (parakeet): 'resources that flow through our gates were never meant
for us belters work the docks loading and unloading precious cargo we fix the 
pipes and filters that keep this rock living and breathing we belters toil and 
suffer without hope and without end and for what one day mars will use its 
might to wrest control of ceres from earth and earth will go to war'
Ref example (The Expanse - S01E01.srt): 'were never meant for us. Belters work 
the docks, l'...
Ref example (The Expanse - S01E02.srt): "- Hey! Hey! Hey! - Whoa, whoa! Let's 
just take a d"...
Ref example (The Expanse - S01E03.srt): "Oh, no. It's important. Now. The <i> 
scopuli</i> i"...
Ref example (The Expanse - S01E04.srt): 'Torpedoes are away! Guidance lock on 
all targets. '...
Ref example (The Expanse - S01E05.srt): 'Oh, hoss. She is one beautiful lady. 
She purrs lik'...
Ref example (The Expanse - S01E06.srt): 'At least you think you are... If I 
wanted to hurt '...
Ref example (The Expanse - S01E07.srt): "It's a radio transmission coming from 
between the "...
Ref example (The Expanse - S01E08.srt): '_ Only thing we got here is the lovely
charted bel'...
Ref example (The Expanse - S01E09.srt): 'Breaching pod, amidships! Seal all 
compartments! A'...
Ref example (The Expanse - S01E10.srt): "In here. This way. Let's go. Get over 
there. Pleas"...
Best Confidence: 0.84

Testing whisper:tiny.en...
Identifying The Expanse - S01E01.mkv (S1)...
Found 10 reference files.
Processing chunk at 300s...
Transcribed (whisper): ' resources that flow through our gates would never mend
for us. Bell tos work the docks, loading and unloading precious cargo. We fix 
the pipes and filters that keep this rock living and breathing. We belt as Toyo
and Safa. Without hope and without end and for what? One day, Mars will use its
might to rest control of series from her and earth will go to the wall.'
Ref example (The Expanse - S01E01.srt): 'were never meant for us. Belters work 
the docks, l'...
Ref example (The Expanse - S01E02.srt): "- Hey! Hey! Hey! - Whoa, whoa! Let's 
just take a d"...
Ref example (The Expanse - S01E03.srt): "Oh, no. It's important. Now. The <i> 
scopuli</i> i"...
Ref example (The Expanse - S01E04.srt): 'Torpedoes are away! Guidance lock on 
all targets. '...
Ref example (The Expanse - S01E05.srt): 'Oh, hoss. She is one beautiful lady. 
She purrs lik'...
Ref example (The Expanse - S01E06.srt): 'At least you think you are... If I 
wanted to hurt '...
Ref example (The Expanse - S01E07.srt): "It's a radio transmission coming from 
between the "...
Ref example (The Expanse - S01E08.srt): '_ Only thing we got here is the lovely
charted bel'...
Ref example (The Expanse - S01E09.srt): 'Breaching pod, amidships! Seal all 
compartments! A'...
Ref example (The Expanse - S01E10.srt): "In here. This way. Let's go. Get over 
there. Pleas"...
Best Confidence: 0.78
                               Benchmark Results                               
+-----------------------------------------------------------------------------+
| Model                         | Time (s) | Status | Confidence | Matched Ep |
|-------------------------------+----------+--------+------------+------------|
| parakeet:nvidia/parakeet-ctc√† | 63.63    | MATCH  | 0.84       | S1E1       |
| whisper:tiny.en               | 2.57     | MATCH  | 0.78       | S1E1       |
+-----------------------------------------------------------------------------+
