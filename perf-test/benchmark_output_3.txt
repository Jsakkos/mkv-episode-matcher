2025-12-25 22:30:04.198 | INFO     | mkv_episode_matcher.config:<module>:19 - Total available threads: 12 -> Setting max to 4
2025-12-25 22:30:04.201 | INFO     | mkv_episode_matcher.__main__:<module>:18 - Starting the application
+-----------------------------------------------+
| Simplified GPU Benchmark: Whisper vs Parakeet |
+-----------------------------------------------+
CUDA Device: NVIDIA GeForce RTX 3090

Test File: Rick and Morty - S01E01.mkv
Detected Show Name: Rick and Morty
Target: S01E01
Expected Cache Path: C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and
Morty
Cache directory exists.
Found 122 SRT files.

Testing whisper:tiny.en...
Identifying Rick and Morty - S01E01.mkv (S1)...
Debug: Matcher looking in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and Morty
Debug: Search patterns: ['S01E', 'S1E', '01x', '1x']
Debug: First file in dir: Rick and Morty - S01E01.srt
Debug: Pattern S01E MATCHES Rick and Morty - S01E01.srt
Debug: Pattern S1E does NOT match Rick and Morty - S01E01.srt
Debug: Pattern 01x does NOT match Rick and Morty - S01E01.srt
Debug: Pattern 1x does NOT match Rick and Morty - S01E01.srt
Found 11 reference files.
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.44)
Season: 1, Episode: 1 (confidence: 0.44)

Testing parakeet:nvidia/parakeet-tdt-0.6b-v2...
Identifying Rick and Morty - S01E01.mkv (S1)...
Debug: Matcher looking in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and Morty
Debug: Search patterns: ['S01E', 'S1E', '01x', '1x']
Debug: First file in dir: Rick and Morty - S01E01.srt
Debug: Pattern S01E MATCHES Rick and Morty - S01E01.srt
Debug: Pattern S1E does NOT match Rick and Morty - S01E01.srt
Debug: Pattern 01x does NOT match Rick and Morty - S01E01.srt
Debug: Pattern 1x does NOT match Rick and Morty - S01E01.srt
Found 11 reference files.
[NeMo W 2025-12-25 22:30:23 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.
W1225 22:30:23.891000 36168 Lib\site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.
No exporters were provided. This means that no telemetry data will be collected.
[NeMo I 2025-12-25 22:30:36 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens
[NeMo W 2025-12-25 22:30:37 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    use_lhotse: true
    skip_missing_manifest_entries: true
    input_cfg: null
    tarred_audio_filepaths: null
    manifest_filepath: null
    sample_rate: 16000
    shuffle: true
    num_workers: 2
    pin_memory: true
    max_duration: 40.0
    min_duration: 0.1
    text_field: answer
    batch_duration: null
    use_bucketing: true
    bucket_duration_bins: null
    bucket_batch_size: null
    num_buckets: 30
    bucket_buffer_size: 20000
    shuffle_buffer_size: 10000
    
[NeMo W 2025-12-25 22:30:37 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    use_lhotse: true
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    max_duration: 40.0
    min_duration: 0.1
    num_workers: 2
    pin_memory: true
    text_field: answer
    
[NeMo I 2025-12-25 22:30:37 nemo_logging:393] PADDING: 0
[NeMo I 2025-12-25 22:30:44 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-12-25 22:30:44 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-12-25 22:30:48 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-12-25 22:30:53 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from C:\Users\Jonathan\.cache\huggingface\hub\models--nvidia--parakeet-tdt-0.6b-v2\snapshots\48b630d20b000e5ad3735e5378a2d9bde3f80826\parakeet-tdt-0.6b-v2.nemo.
[NeMo W 2025-12-25 22:31:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 0it [00:07, ?it/s]
[NeMo W 2025-12-25 22:31:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.35it/s]
Transcribing: 1it [00:00,  2.35it/s]
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.76it/s]
Transcribing: 1it [00:00,  3.74it/s]
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.18it/s]
Transcribing: 1it [00:00,  6.18it/s]
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.17it/s]
Transcribing: 1it [00:00,  8.17it/s]
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.38it/s]
Transcribing: 1it [00:00,  8.31it/s]
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.88it/s]
Transcribing: 1it [00:00,  8.80it/s]
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.63it/s]
Transcribing: 1it [00:00,  9.63it/s]
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.37it/s]
Transcribing: 1it [00:00,  9.32it/s]
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:31:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  7.42it/s]
Transcribing: 1it [00:00,  7.42it/s]
                               Benchmark Results                               
+-----------------------------------------------------------------------------+
| Model                       | Time (s) | Status   | Confidence | Matched Ep |
|-----------------------------+----------+----------+------------+------------|
| whisper:tiny.en             | 10.29    | MATCH    | 0.44       | S1E1       |
| parakeet:nvidia/parakeet-t√† | 63.13    | NO MATCH | 0.00       | N/A        |
+-----------------------------------------------------------------------------+
