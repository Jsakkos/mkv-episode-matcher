2025-12-25 23:45:29.289 | INFO     | mkv_episode_matcher.config:<module>:19 - Total available threads: 12 -> Setting max to 4
2025-12-25 23:45:29.295 | INFO     | mkv_episode_matcher.__main__:<module>:18 - Starting the application
MKV Episode Matcher Performance Benchmark Tool

Debug: generate_ground_truth using 
test_files_dir=D:\mkv-episode-matcher\perf-test\inputs
Debug: generate_ground_truth 
cache_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache
Debug: found file=Rick and Morty - S01E01.mkv -> show_name=Rick and Morty 
season=1 episode=1
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and Morty
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 122 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and Morty
Debug: pattern='S01E' matched 22 files
Added to ground truth: Rick and Morty - S01E01.mkv -> Rick and Morty S1E1
Debug: found file=South Park - s05e01.mkv -> show_name=South Park season=5 
episode=1
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: _has_reference_subtitles will search for patterns: ['S05E', 'S5E', 
'05x', '5x']
Debug: found 88 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: pattern='S05E' matched 28 files
Added to ground truth: South Park - s05e01.mkv -> South Park S5E1
Debug: found file=South Park - s05e02.mkv -> show_name=South Park season=5 
episode=2
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: _has_reference_subtitles will search for patterns: ['S05E', 'S5E', 
'05x', '5x']
Debug: found 88 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: pattern='S05E' matched 28 files
Added to ground truth: South Park - s05e02.mkv -> South Park S5E2
Debug: found file=South Park - s05e03.mkv -> show_name=South Park season=5 
episode=3
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: _has_reference_subtitles will search for patterns: ['S05E', 'S5E', 
'05x', '5x']
Debug: found 88 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park
Debug: pattern='S05E' matched 28 files
Added to ground truth: South Park - s05e03.mkv -> South Park S5E3
Debug: found file=Ted Lasso - S01E01.mkv -> show_name=Ted Lasso season=1 
episode=1
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 66 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso
Debug: pattern='S01E' matched 20 files
Added to ground truth: Ted Lasso - S01E01.mkv -> Ted Lasso S1E1
Debug: found file=The Expanse - S01E01.mkv -> show_name=The Expanse season=1 
episode=1
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 122 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: pattern='S01E' matched 20 files
Added to ground truth: The Expanse - S01E01.mkv -> The Expanse S1E1
Debug: found file=The Expanse - S01E02.mkv -> show_name=The Expanse season=1 
episode=2
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 122 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: pattern='S01E' matched 20 files
Added to ground truth: The Expanse - S01E02.mkv -> The Expanse S1E2
Debug: found file=The Expanse - S01E03.mkv -> show_name=The Expanse season=1 
episode=3
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 122 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: pattern='S01E' matched 20 files
Added to ground truth: The Expanse - S01E03.mkv -> The Expanse S1E3
Debug: found file=The Expanse - S01E04.mkv -> show_name=The Expanse season=1 
episode=4
Debug: _has_reference_subtitles checking 
reference_dir=C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: _has_reference_subtitles will search for patterns: ['S01E', 'S1E', 
'01x', '1x']
Debug: found 122 srt files in 
C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse
Debug: pattern='S01E' matched 20 files
Added to ground truth: The Expanse - S01E04.mkv -> The Expanse S1E4
+-------------------------- Benchmark Configuration --------------------------+
| MKV Episode Matcher Performance Benchmark                                   |
| Test files: 9                                                               |
| Iterations per file: 3                                                      |
| Models to test: whisper:tiny.en, parakeet:nvidia/parakeet-ctc-0.6b,         |
| parakeet:nvidia/parakeet-ctc-1.1b, faster-whisper:tiny.en,                  |
| faster-whisper:distil-large-v3                                              |
| Devices to test: cuda                                                       |
+-----------------------------------------------------------------------------+

Starting benchmark of 9 files across 1 device(s) and 5 model(s)...


Testing on device: cuda

Testing model: whisper:tiny.en
(1/45) Rick and Morty - S01E01.mkv on cuda with whisper:tiny.en
Benchmarking: Rick and Morty - S01E01.mkv (device: cuda, model: 
whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.87)
Season: 1, Episode: 1 (confidence: 0.87)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.87)
Season: 1, Episode: 1 (confidence: 0.87)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.87)
Season: 1, Episode: 1 (confidence: 0.87)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.87)
Season: 1, Episode: 1 (confidence: 0.87)
  OK Rick and Morty - S01E01.mkv (cuda, whisper:tiny.en): 4.79s avg, 100.00% 
accuracy

(2/45) South Park - s05e01.mkv on cuda with whisper:tiny.en
Benchmarking: South Park - s05e01.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  OK South Park - s05e01.mkv (cuda, whisper:tiny.en): 7.08s avg, 100.00% 
accuracy

(3/45) South Park - s05e02.mkv on cuda with whisper:tiny.en
Benchmarking: South Park - s05e02.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  OK South Park - s05e02.mkv (cuda, whisper:tiny.en): 8.70s avg, 0.00% accuracy

(4/45) South Park - s05e03.mkv on cuda with whisper:tiny.en
Benchmarking: South Park - s05e03.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.78)
Season: 5, Episode: 1 (confidence: 0.78)
  OK South Park - s05e03.mkv (cuda, whisper:tiny.en): 7.95s avg, 0.00% accuracy

(5/45) Ted Lasso - S01E01.mkv on cuda with whisper:tiny.en
Benchmarking: Ted Lasso - S01E01.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.84)
Season: 1, Episode: 1 (confidence: 0.84)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.84)
Season: 1, Episode: 1 (confidence: 0.84)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.84)
Season: 1, Episode: 1 (confidence: 0.84)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.84)
Season: 1, Episode: 1 (confidence: 0.84)
  OK Ted Lasso - S01E01.mkv (cuda, whisper:tiny.en): 10.43s avg, 100.00% 
accuracy

(6/45) The Expanse - S01E01.mkv on cuda with whisper:tiny.en
Benchmarking: The Expanse - S01E01.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.61)
Season: 1, Episode: 1 (confidence: 0.61)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  OK The Expanse - S01E01.mkv (cuda, whisper:tiny.en): 13.06s avg, 100.00% 
accuracy

(7/45) The Expanse - S01E02.mkv on cuda with whisper:tiny.en
Benchmarking: The Expanse - S01E02.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.66)
Season: 1, Episode: 1 (confidence: 0.66)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  OK The Expanse - S01E02.mkv (cuda, whisper:tiny.en): 12.07s avg, 0.00% 
accuracy

(8/45) The Expanse - S01E03.mkv on cuda with whisper:tiny.en
Benchmarking: The Expanse - S01E03.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.65)
Season: 1, Episode: 1 (confidence: 0.65)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E03.srt (confidence: 0.91)
Season: 1, Episode: 3 (confidence: 0.91)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.65)
Season: 1, Episode: 1 (confidence: 0.65)
  OK The Expanse - S01E03.mkv (cuda, whisper:tiny.en): 16.10s avg, 33.33% 
accuracy

(9/45) The Expanse - S01E04.mkv on cuda with whisper:tiny.en
Benchmarking: The Expanse - S01E04.mkv (device: cuda, model: whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.65)
Season: 1, Episode: 1 (confidence: 0.65)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.64)
Season: 1, Episode: 1 (confidence: 0.64)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.65)
Season: 1, Episode: 1 (confidence: 0.65)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.65)
Season: 1, Episode: 1 (confidence: 0.65)
  OK The Expanse - S01E04.mkv (cuda, whisper:tiny.en): 11.75s avg, 0.00% 
accuracy


Testing model: parakeet:nvidia/parakeet-ctc-0.6b
(10/45) Rick and Morty - S01E01.mkv on cuda with 
parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: Rick and Morty - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:52:04 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.
W1225 23:52:04.339000 38696 Lib\site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.
No exporters were provided. This means that no telemetry data will be collected.
[NeMo I 2025-12-25 23:52:18 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens
[NeMo W 2025-12-25 23:52:18 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-train-all.json
    sample_rate: 16000
    batch_size: 16
    shuffle: true
    num_workers: 8
    pin_memory: true
    use_start_end_token: false
    trim_silence: false
    max_duration: 16.7
    min_duration: 0.1
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    bucketing_strategy: fully_randomized
    bucketing_batch_size: null
    
[NeMo W 2025-12-25 23:52:18 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-dev-clean.json
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo W 2025-12-25 23:52:18 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
    Test config : 
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo I 2025-12-25 23:52:18 nemo_logging:393] PADDING: 0
[NeMo I 2025-12-25 23:52:30 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from C:\Users\Jonathan\.cache\huggingface\hub\models--nvidia--parakeet-ctc-0.6b\snapshots\ad09ba1cc62743fbc9814de5d2016fca9096485a\parakeet-ctc-0.6b.nemo.
[NeMo W 2025-12-25 23:52:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s][NeMo W 2025-12-25 23:52:40 nemo_logging:405] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.

Transcribing: 1it [00:03,  3.49s/it]
Transcribing: 1it [00:03,  3.49s/it]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.76)
Season: 1, Episode: 1 (confidence: 0.76)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:52:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.22it/s]
Transcribing: 1it [00:00,  4.22it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.76)
Season: 1, Episode: 1 (confidence: 0.76)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:52:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 10.36it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.76)
Season: 1, Episode: 1 (confidence: 0.76)
  Running profiled test...
[NeMo W 2025-12-25 23:52:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.46it/s]
Transcribing: 1it [00:00,  2.45it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.76)
Season: 1, Episode: 1 (confidence: 0.76)
  OK Rick and Morty - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 
20.07s avg, 100.00% accuracy

(11/45) South Park - s05e01.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: South Park - s05e01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:52:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.68it/s]
Transcribing: 1it [00:00,  4.66it/s]
[NeMo W 2025-12-25 23:52:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.63it/s]
Transcribing: 1it [00:00,  2.62it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:52:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.05it/s]
Transcribing: 1it [00:00,  4.04it/s]
[NeMo W 2025-12-25 23:52:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.29it/s]
Transcribing: 1it [00:00,  3.29it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:52:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.12it/s]
Transcribing: 1it [00:00,  3.12it/s]
[NeMo W 2025-12-25 23:52:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.63it/s]
Transcribing: 1it [00:00,  2.63it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:52:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.02it/s]
Transcribing: 1it [00:00,  2.01it/s]
[NeMo W 2025-12-25 23:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.96it/s]
Transcribing: 1it [00:00,  3.94it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e01.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.27s 
avg, 100.00% accuracy

(12/45) South Park - s05e02.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: South Park - s05e02.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:52:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.44it/s]
Transcribing: 1it [00:00,  3.44it/s]
[NeMo W 2025-12-25 23:52:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.09it/s]
Transcribing: 1it [00:00,  3.09it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:52:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.83it/s]
Transcribing: 1it [00:00,  3.83it/s]
[NeMo W 2025-12-25 23:52:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.19it/s]
Transcribing: 1it [00:00,  3.18it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:52:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.43it/s]
Transcribing: 1it [00:00,  2.43it/s]
[NeMo W 2025-12-25 23:52:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.49it/s]
Transcribing: 1it [00:00,  2.49it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:52:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.96it/s]
Transcribing: 1it [00:00,  2.95it/s]
[NeMo W 2025-12-25 23:52:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.56it/s]
Transcribing: 1it [00:00,  2.56it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e02.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.34s 
avg, 0.00% accuracy

(13/45) South Park - s05e03.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: South Park - s05e03.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:52:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.07it/s]
Transcribing: 1it [00:00,  3.07it/s]
[NeMo W 2025-12-25 23:52:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.87it/s]
Transcribing: 1it [00:00,  3.85it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:52:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.73it/s]
Transcribing: 1it [00:00,  2.73it/s]
[NeMo W 2025-12-25 23:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.67it/s]
Transcribing: 1it [00:00,  5.67it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:52:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:52:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.39it/s]
Transcribing: 1it [00:00,  4.39it/s]
[NeMo W 2025-12-25 23:53:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.91it/s]
Transcribing: 1it [00:00,  2.91it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:53:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.52it/s]
Transcribing: 1it [00:00,  2.51it/s]
[NeMo W 2025-12-25 23:53:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.14it/s]
Transcribing: 1it [00:00,  3.12it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e03.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.24s 
avg, 0.00% accuracy

(14/45) Ted Lasso - S01E01.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: Ted Lasso - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:53:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.53it/s]
Transcribing: 1it [00:00,  2.52it/s]
[NeMo W 2025-12-25 23:53:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.66it/s]
Transcribing: 1it [00:00,  3.66it/s]
[NeMo W 2025-12-25 23:53:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.89it/s]
Transcribing: 1it [00:00,  2.88it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.81)
Season: 1, Episode: 1 (confidence: 0.81)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:53:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.84it/s]
Transcribing: 1it [00:00,  4.84it/s]
[NeMo W 2025-12-25 23:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.23it/s]
Transcribing: 1it [00:00,  2.23it/s]
[NeMo W 2025-12-25 23:53:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.87it/s]
Transcribing: 1it [00:00,  2.87it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.81)
Season: 1, Episode: 1 (confidence: 0.81)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:53:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.20it/s]
Transcribing: 1it [00:00,  4.19it/s]
[NeMo W 2025-12-25 23:53:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.55it/s]
Transcribing: 1it [00:00,  4.53it/s]
[NeMo W 2025-12-25 23:53:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.06it/s]
Transcribing: 1it [00:00,  4.06it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.81)
Season: 1, Episode: 1 (confidence: 0.81)
  Running profiled test...
[NeMo W 2025-12-25 23:53:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.76it/s]
Transcribing: 1it [00:00,  2.75it/s]
[NeMo W 2025-12-25 23:53:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.93it/s]
Transcribing: 1it [00:00,  2.92it/s]
[NeMo W 2025-12-25 23:53:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.46it/s]
Transcribing: 1it [00:00,  2.45it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.81)
Season: 1, Episode: 1 (confidence: 0.81)
  OK Ted Lasso - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.50s 
avg, 100.00% accuracy

(15/45) The Expanse - S01E01.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: The Expanse - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:53:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.75it/s]
Transcribing: 1it [00:00,  2.75it/s]
[NeMo W 2025-12-25 23:53:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.24it/s]
Transcribing: 1it [00:00,  3.23it/s]
[NeMo W 2025-12-25 23:53:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.65it/s]
Transcribing: 1it [00:00,  3.65it/s]
[NeMo W 2025-12-25 23:53:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.36it/s]
Transcribing: 1it [00:00,  2.36it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:53:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.56it/s]
Transcribing: 1it [00:00,  4.52it/s]
[NeMo W 2025-12-25 23:53:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.26it/s]
Transcribing: 1it [00:00,  2.25it/s]
[NeMo W 2025-12-25 23:53:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.32it/s]
Transcribing: 1it [00:00,  2.32it/s]
[NeMo W 2025-12-25 23:53:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.29it/s]
Transcribing: 1it [00:00,  4.27it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:53:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  1.71it/s]
Transcribing: 1it [00:00,  1.71it/s]
[NeMo W 2025-12-25 23:53:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.25it/s]
Transcribing: 1it [00:00,  4.21it/s]
[NeMo W 2025-12-25 23:53:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.13it/s]
Transcribing: 1it [00:00,  3.12it/s]
[NeMo W 2025-12-25 23:53:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.42it/s]
Transcribing: 1it [00:00,  2.42it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running profiled test...
[NeMo W 2025-12-25 23:53:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.92it/s]
Transcribing: 1it [00:00,  2.91it/s]
[NeMo W 2025-12-25 23:53:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.59it/s]
Transcribing: 1it [00:00,  3.57it/s]
[NeMo W 2025-12-25 23:53:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.53it/s]
Transcribing: 1it [00:00,  2.52it/s]
[NeMo W 2025-12-25 23:53:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.66it/s]
Transcribing: 1it [00:00,  2.65it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  OK The Expanse - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 2.58s 
avg, 100.00% accuracy

(16/45) The Expanse - S01E02.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: The Expanse - S01E02.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:53:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.45it/s]
Transcribing: 1it [00:00,  4.43it/s]
[NeMo W 2025-12-25 23:53:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.35it/s]
Transcribing: 1it [00:00,  2.35it/s]
[NeMo W 2025-12-25 23:53:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.19it/s]
Transcribing: 1it [00:00,  3.19it/s]
[NeMo W 2025-12-25 23:53:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.55it/s]
Transcribing: 1it [00:00,  2.54it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:53:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.79it/s]
Transcribing: 1it [00:00,  2.79it/s]
[NeMo W 2025-12-25 23:53:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.50it/s]
Transcribing: 1it [00:00,  3.49it/s]
[NeMo W 2025-12-25 23:53:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.46it/s]
Transcribing: 1it [00:00,  2.46it/s]
[NeMo W 2025-12-25 23:53:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.51it/s]
Transcribing: 1it [00:00,  2.51it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:53:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.87it/s]
Transcribing: 1it [00:00,  3.87it/s]
[NeMo W 2025-12-25 23:53:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.61it/s]
Transcribing: 1it [00:00,  3.60it/s]
[NeMo W 2025-12-25 23:53:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.18it/s]
Transcribing: 1it [00:00,  4.18it/s]
[NeMo W 2025-12-25 23:53:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.59it/s]
Transcribing: 1it [00:00,  6.59it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running profiled test...
[NeMo W 2025-12-25 23:53:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.79it/s]
Transcribing: 1it [00:00,  3.79it/s]
[NeMo W 2025-12-25 23:53:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  1.97it/s]
Transcribing: 1it [00:00,  1.96it/s]
[NeMo W 2025-12-25 23:53:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.97it/s]
Transcribing: 1it [00:00,  3.95it/s]
[NeMo W 2025-12-25 23:53:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.54it/s]
Transcribing: 1it [00:00,  3.53it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  OK The Expanse - S01E02.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 2.07s 
avg, 0.00% accuracy

(17/45) The Expanse - S01E03.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: The Expanse - S01E03.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:53:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.98it/s]
Transcribing: 1it [00:00,  2.98it/s]
[NeMo W 2025-12-25 23:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.95it/s]
Transcribing: 1it [00:00,  2.94it/s]
[NeMo W 2025-12-25 23:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.66it/s]
Transcribing: 1it [00:00,  3.66it/s]
[NeMo W 2025-12-25 23:53:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.83it/s]
Transcribing: 1it [00:00,  2.82it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:53:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.29it/s]
Transcribing: 1it [00:00,  2.29it/s]
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.63it/s]
Transcribing: 1it [00:00,  3.63it/s]
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  7.94it/s]
Transcribing: 1it [00:00,  7.94it/s]
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 11.31it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 12.41it/s]
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 10.43it/s]
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 11.05it/s]
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00, 11.10it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running profiled test...
[NeMo W 2025-12-25 23:53:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.57it/s]
Transcribing: 1it [00:00,  9.48it/s]
[NeMo W 2025-12-25 23:53:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.38it/s]
Transcribing: 1it [00:00,  9.30it/s]
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.29it/s]
Transcribing: 1it [00:00,  8.22it/s]
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.42it/s]
Transcribing: 1it [00:00,  9.42it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  OK The Expanse - S01E03.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.55s 
avg, 0.00% accuracy

(18/45) The Expanse - S01E04.mkv on cuda with parakeet:nvidia/parakeet-ctc-0.6b
Benchmarking: The Expanse - S01E04.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-0.6b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.73it/s]
Transcribing: 1it [00:00,  9.73it/s]
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.83it/s]
Transcribing: 1it [00:00,  4.83it/s]
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  7.40it/s]
Transcribing: 1it [00:00,  7.35it/s]
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.47it/s]
Transcribing: 1it [00:00,  8.47it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.77it/s]
Transcribing: 1it [00:00,  8.69it/s]
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.79it/s]
Transcribing: 1it [00:00,  9.79it/s]
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.01it/s]
Transcribing: 1it [00:00,  8.93it/s]
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  7.89it/s]
Transcribing: 1it [00:00,  7.83it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.19it/s]
Transcribing: 1it [00:00,  8.12it/s]
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.68it/s]
Transcribing: 1it [00:00,  8.68it/s]
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.98it/s]
Transcribing: 1it [00:00,  8.98it/s]
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  9.92it/s]
Transcribing: 1it [00:00,  9.92it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  Running profiled test...
[NeMo W 2025-12-25 23:53:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  7.12it/s]
Transcribing: 1it [00:00,  7.07it/s]
[NeMo W 2025-12-25 23:53:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  8.43it/s]
Transcribing: 1it [00:00,  8.43it/s]
[NeMo W 2025-12-25 23:53:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.87it/s]
Transcribing: 1it [00:00,  4.85it/s]
[NeMo W 2025-12-25 23:53:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:53:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.77it/s]
Transcribing: 1it [00:00,  6.73it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.69)
Season: 1, Episode: 1 (confidence: 0.69)
  OK The Expanse - S01E04.mkv (cuda, parakeet:nvidia/parakeet-ctc-0.6b): 1.05s 
avg, 0.00% accuracy


Testing model: parakeet:nvidia/parakeet-ctc-1.1b
(19/45) Rick and Morty - S01E01.mkv on cuda with 
parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: Rick and Morty - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
[NeMo I 2025-12-25 23:54:51 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens
[NeMo W 2025-12-25 23:54:52 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-train-all.json
    sample_rate: 16000
    batch_size: 16
    shuffle: true
    num_workers: 8
    pin_memory: true
    use_start_end_token: false
    trim_silence: false
    max_duration: 16.7
    min_duration: 0.1
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    bucketing_strategy: fully_randomized
    bucketing_batch_size: null
    
[NeMo W 2025-12-25 23:54:52 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    manifest_filepath: /disk1/NVIDIA/datasets/LibriSpeech_NeMo/librivox-dev-clean.json
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo W 2025-12-25 23:54:52 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
    Test config : 
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    use_start_end_token: false
    num_workers: 8
    pin_memory: true
    
[NeMo I 2025-12-25 23:54:52 nemo_logging:393] PADDING: 0
[NeMo I 2025-12-25 23:55:12 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from C:\Users\Jonathan\.cache\huggingface\hub\models--nvidia--parakeet-ctc-1.1b\snapshots\a707e818195cb97c8f7da2fc36b221a29f69a5db\parakeet-ctc-1.1b.nemo.
[NeMo W 2025-12-25 23:55:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.84it/s]
Transcribing: 1it [00:00,  5.84it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.75)
Season: 1, Episode: 1 (confidence: 0.75)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.45it/s]
Transcribing: 1it [00:00,  6.41it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.75)
Season: 1, Episode: 1 (confidence: 0.75)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.03it/s]
Transcribing: 1it [00:00,  5.99it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.75)
Season: 1, Episode: 1 (confidence: 0.75)
  Running profiled test...
[NeMo W 2025-12-25 23:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.21it/s]
Transcribing: 1it [00:00,  5.18it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.75)
Season: 1, Episode: 1 (confidence: 0.75)
  OK Rick and Morty - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 
30.67s avg, 100.00% accuracy

(20/45) South Park - s05e01.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: South Park - s05e01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.80it/s]
Transcribing: 1it [00:00,  4.78it/s]
[NeMo W 2025-12-25 23:55:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.16it/s]
Transcribing: 1it [00:00,  6.16it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.03it/s]
Transcribing: 1it [00:00,  6.03it/s]
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.88it/s]
Transcribing: 1it [00:00,  5.88it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.37it/s]
Transcribing: 1it [00:00,  6.37it/s]
[NeMo W 2025-12-25 23:55:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.96it/s]
Transcribing: 1it [00:00,  5.96it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:55:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.63it/s]
Transcribing: 1it [00:00,  4.61it/s]
[NeMo W 2025-12-25 23:55:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.82it/s]
Transcribing: 1it [00:00,  5.79it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e01.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 0.97s 
avg, 100.00% accuracy

(21/45) South Park - s05e02.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: South Park - s05e02.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.30it/s]
Transcribing: 1it [00:00,  5.30it/s]
[NeMo W 2025-12-25 23:55:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.57it/s]
Transcribing: 1it [00:00,  5.56it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.67it/s]
Transcribing: 1it [00:00,  5.67it/s]
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.41it/s]
Transcribing: 1it [00:00,  5.38it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.67it/s]
Transcribing: 1it [00:00,  6.62it/s]
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.50it/s]
Transcribing: 1it [00:00,  6.45it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:55:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.12it/s]
Transcribing: 1it [00:00,  5.12it/s]
[NeMo W 2025-12-25 23:55:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.25it/s]
Transcribing: 1it [00:00,  5.25it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e02.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 0.95s 
avg, 0.00% accuracy

(22/45) South Park - s05e03.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: South Park - s05e03.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.92it/s]
Transcribing: 1it [00:00,  5.92it/s]
[NeMo W 2025-12-25 23:55:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.47it/s]
Transcribing: 1it [00:00,  5.47it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.64it/s]
Transcribing: 1it [00:00,  4.64it/s]
[NeMo W 2025-12-25 23:55:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.37it/s]
Transcribing: 1it [00:00,  5.37it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.34it/s]
Transcribing: 1it [00:00,  6.30it/s]
[NeMo W 2025-12-25 23:55:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.16it/s]
Transcribing: 1it [00:00,  6.16it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
[NeMo W 2025-12-25 23:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.88it/s]
Transcribing: 1it [00:00,  5.84it/s]
[NeMo W 2025-12-25 23:55:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.88it/s]
Transcribing: 1it [00:00,  4.84it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e03.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 1.00s 
avg, 0.00% accuracy

(23/45) Ted Lasso - S01E01.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: Ted Lasso - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.51it/s]
Transcribing: 1it [00:00,  6.47it/s]
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.68it/s]
Transcribing: 1it [00:00,  6.63it/s]
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.58it/s]
Transcribing: 1it [00:00,  5.55it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.82)
Season: 1, Episode: 1 (confidence: 0.82)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.57it/s]
Transcribing: 1it [00:00,  4.57it/s]
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.54it/s]
Transcribing: 1it [00:00,  5.54it/s]
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.62it/s]
Transcribing: 1it [00:00,  5.59it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.82)
Season: 1, Episode: 1 (confidence: 0.82)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.12it/s]
Transcribing: 1it [00:00,  5.12it/s]
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.82it/s]
Transcribing: 1it [00:00,  5.82it/s]
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.51it/s]
Transcribing: 1it [00:00,  6.51it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.82)
Season: 1, Episode: 1 (confidence: 0.82)
  Running profiled test...
[NeMo W 2025-12-25 23:55:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.23it/s]
Transcribing: 1it [00:00,  5.20it/s]
[NeMo W 2025-12-25 23:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.63it/s]
Transcribing: 1it [00:00,  5.63it/s]
[NeMo W 2025-12-25 23:55:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.11it/s]
Transcribing: 1it [00:00,  5.09it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.82)
Season: 1, Episode: 1 (confidence: 0.82)
  OK Ted Lasso - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 1.08s 
avg, 100.00% accuracy

(24/45) The Expanse - S01E01.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: The Expanse - S01E01.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.94it/s]
Transcribing: 1it [00:00,  5.94it/s]
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.33it/s]
Transcribing: 1it [00:00,  5.30it/s]
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.08it/s]
Transcribing: 1it [00:00,  5.06it/s]
[NeMo W 2025-12-25 23:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.01it/s]
Transcribing: 1it [00:00,  5.98it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.11it/s]
Transcribing: 1it [00:00,  5.08it/s]
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.12it/s]
Transcribing: 1it [00:00,  5.12it/s]
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  6.07it/s]
Transcribing: 1it [00:00,  6.05it/s]
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  5.33it/s]
Transcribing: 1it [00:00,  5.30it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.27it/s]
Transcribing: 1it [00:00,  3.26it/s]
[NeMo W 2025-12-25 23:55:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.21it/s]
Transcribing: 1it [00:00,  3.21it/s]
[NeMo W 2025-12-25 23:55:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.11it/s]
Transcribing: 1it [00:00,  2.10it/s]
[NeMo W 2025-12-25 23:55:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.00it/s]
Transcribing: 1it [00:00,  3.00it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running profiled test...
[NeMo W 2025-12-25 23:55:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.83it/s]
Transcribing: 1it [00:00,  2.82it/s]
[NeMo W 2025-12-25 23:55:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.44it/s]
Transcribing: 1it [00:00,  2.44it/s]
[NeMo W 2025-12-25 23:55:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.57it/s]
Transcribing: 1it [00:00,  2.56it/s]
[NeMo W 2025-12-25 23:55:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.15it/s]
Transcribing: 1it [00:00,  3.14it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  OK The Expanse - S01E01.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 1.74s 
avg, 100.00% accuracy

(25/45) The Expanse - S01E02.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: The Expanse - S01E02.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.06it/s]
Transcribing: 1it [00:00,  3.06it/s]
[NeMo W 2025-12-25 23:55:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.37it/s]
Transcribing: 1it [00:00,  2.37it/s]
[NeMo W 2025-12-25 23:55:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.21it/s]
Transcribing: 1it [00:00,  2.21it/s]
[NeMo W 2025-12-25 23:55:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.00it/s]
Transcribing: 1it [00:00,  2.99it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.34it/s]
Transcribing: 1it [00:00,  2.33it/s]
[NeMo W 2025-12-25 23:55:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.22it/s]
Transcribing: 1it [00:00,  2.22it/s]
[NeMo W 2025-12-25 23:55:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.32it/s]
Transcribing: 1it [00:00,  2.32it/s]
[NeMo W 2025-12-25 23:55:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.12it/s]
Transcribing: 1it [00:00,  2.12it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.29it/s]
Transcribing: 1it [00:00,  4.28it/s]
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.13it/s]
Transcribing: 1it [00:00,  3.13it/s]
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.75it/s]
Transcribing: 1it [00:00,  4.73it/s]
[NeMo W 2025-12-25 23:55:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.59it/s]
Transcribing: 1it [00:00,  2.59it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running profiled test...
[NeMo W 2025-12-25 23:55:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.87it/s]
Transcribing: 1it [00:00,  2.86it/s]
[NeMo W 2025-12-25 23:55:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.82it/s]
Transcribing: 1it [00:00,  2.82it/s]
[NeMo W 2025-12-25 23:55:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.80it/s]
Transcribing: 1it [00:00,  2.80it/s]
[NeMo W 2025-12-25 23:55:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.65it/s]
Transcribing: 1it [00:00,  2.64it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  OK The Expanse - S01E02.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 2.32s 
avg, 0.00% accuracy

(26/45) The Expanse - S01E03.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: The Expanse - S01E03.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:55:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.03it/s]
Transcribing: 1it [00:00,  3.02it/s]
[NeMo W 2025-12-25 23:55:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.86it/s]
Transcribing: 1it [00:00,  2.86it/s]
[NeMo W 2025-12-25 23:55:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.55it/s]
Transcribing: 1it [00:00,  2.55it/s]
[NeMo W 2025-12-25 23:55:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.49it/s]
Transcribing: 1it [00:00,  2.49it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:55:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.79it/s]
Transcribing: 1it [00:00,  2.79it/s]
[NeMo W 2025-12-25 23:55:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.39it/s]
Transcribing: 1it [00:00,  2.38it/s]
[NeMo W 2025-12-25 23:55:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.44it/s]
Transcribing: 1it [00:00,  2.43it/s]
[NeMo W 2025-12-25 23:55:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.25it/s]
Transcribing: 1it [00:00,  3.25it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:55:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.27it/s]
Transcribing: 1it [00:00,  3.27it/s]
[NeMo W 2025-12-25 23:55:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:55:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  1.89it/s]
Transcribing: 1it [00:00,  1.89it/s]
[NeMo W 2025-12-25 23:56:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.76it/s]
Transcribing: 1it [00:00,  2.76it/s]
[NeMo W 2025-12-25 23:56:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.67it/s]
Transcribing: 1it [00:00,  2.67it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running profiled test...
[NeMo W 2025-12-25 23:56:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.23it/s]
Transcribing: 1it [00:00,  4.21it/s]
[NeMo W 2025-12-25 23:56:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.65it/s]
Transcribing: 1it [00:00,  3.64it/s]
[NeMo W 2025-12-25 23:56:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.19it/s]
Transcribing: 1it [00:00,  4.17it/s]
[NeMo W 2025-12-25 23:56:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.59it/s]
Transcribing: 1it [00:00,  2.59it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  OK The Expanse - S01E03.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 2.38s 
avg, 0.00% accuracy

(27/45) The Expanse - S01E04.mkv on cuda with parakeet:nvidia/parakeet-ctc-1.1b
Benchmarking: The Expanse - S01E04.mkv (device: cuda, model: 
parakeet:nvidia/parakeet-ctc-1.1b)
  Running iteration 1/3 on cuda...
[NeMo W 2025-12-25 23:56:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.20it/s]
Transcribing: 1it [00:00,  3.20it/s]
[NeMo W 2025-12-25 23:56:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.14it/s]
Transcribing: 1it [00:00,  2.14it/s]
[NeMo W 2025-12-25 23:56:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.29it/s]
Transcribing: 1it [00:00,  2.29it/s]
[NeMo W 2025-12-25 23:56:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.00it/s]
Transcribing: 1it [00:00,  3.00it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 2/3 on cuda...
[NeMo W 2025-12-25 23:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.62it/s]
Transcribing: 1it [00:00,  2.61it/s]
[NeMo W 2025-12-25 23:56:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.06it/s]
Transcribing: 1it [00:00,  3.06it/s]
[NeMo W 2025-12-25 23:56:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.03it/s]
Transcribing: 1it [00:00,  3.03it/s]
[NeMo W 2025-12-25 23:56:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.44it/s]
Transcribing: 1it [00:00,  2.43it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running iteration 3/3 on cuda...
[NeMo W 2025-12-25 23:56:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.73it/s]
Transcribing: 1it [00:00,  3.71it/s]
[NeMo W 2025-12-25 23:56:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  4.14it/s]
Transcribing: 1it [00:00,  4.14it/s]
[NeMo W 2025-12-25 23:56:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.36it/s]
Transcribing: 1it [00:00,  2.36it/s]
[NeMo W 2025-12-25 23:56:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.87it/s]
Transcribing: 1it [00:00,  2.87it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  Running profiled test...
[NeMo W 2025-12-25 23:56:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  3.05it/s]
Transcribing: 1it [00:00,  3.04it/s]
[NeMo W 2025-12-25 23:56:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.64it/s]
Transcribing: 1it [00:00,  2.64it/s]
[NeMo W 2025-12-25 23:56:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.80it/s]
Transcribing: 1it [00:00,  2.80it/s]
[NeMo W 2025-12-25 23:56:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 23:56:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:00,  2.69it/s]
Transcribing: 1it [00:00,  2.68it/s]
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.70)
Season: 1, Episode: 1 (confidence: 0.70)
  OK The Expanse - S01E04.mkv (cuda, parakeet:nvidia/parakeet-ctc-1.1b): 2.31s 
avg, 0.00% accuracy


Testing model: faster-whisper:tiny.en
(28/45) Rick and Morty - S01E01.mkv on cuda with faster-whisper:tiny.en
Benchmarking: Rick and Morty - S01E01.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.88)
Season: 1, Episode: 1 (confidence: 0.88)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.88)
Season: 1, Episode: 1 (confidence: 0.88)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.88)
Season: 1, Episode: 1 (confidence: 0.88)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.88)
Season: 1, Episode: 1 (confidence: 0.88)
  OK Rick and Morty - S01E01.mkv (cuda, faster-whisper:tiny.en): 5.68s avg, 
100.00% accuracy

(29/45) South Park - s05e01.mkv on cuda with faster-whisper:tiny.en
Benchmarking: South Park - s05e01.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e01.mkv (cuda, faster-whisper:tiny.en): 3.07s avg, 100.00%
accuracy

(30/45) South Park - s05e02.mkv on cuda with faster-whisper:tiny.en
Benchmarking: South Park - s05e02.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e02.mkv (cuda, faster-whisper:tiny.en): 3.19s avg, 0.00% 
accuracy

(31/45) South Park - s05e03.mkv on cuda with faster-whisper:tiny.en
Benchmarking: South Park - s05e03.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.74)
Season: 5, Episode: 1 (confidence: 0.74)
  OK South Park - s05e03.mkv (cuda, faster-whisper:tiny.en): 2.92s avg, 0.00% 
accuracy

(32/45) Ted Lasso - S01E01.mkv on cuda with faster-whisper:tiny.en
Benchmarking: Ted Lasso - S01E01.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.77)
Season: 1, Episode: 1 (confidence: 0.77)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.77)
Season: 1, Episode: 1 (confidence: 0.77)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.77)
Season: 1, Episode: 1 (confidence: 0.77)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.73)
Season: 1, Episode: 1 (confidence: 0.73)
  OK Ted Lasso - S01E01.mkv (cuda, faster-whisper:tiny.en): 10.10s avg, 100.00%
accuracy

(33/45) The Expanse - S01E01.mkv on cuda with faster-whisper:tiny.en
Benchmarking: The Expanse - S01E01.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  OK The Expanse - S01E01.mkv (cuda, faster-whisper:tiny.en): 10.79s avg, 
100.00% accuracy

(34/45) The Expanse - S01E02.mkv on cuda with faster-whisper:tiny.en
Benchmarking: The Expanse - S01E02.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  OK The Expanse - S01E02.mkv (cuda, faster-whisper:tiny.en): 10.15s avg, 0.00%
accuracy

(35/45) The Expanse - S01E03.mkv on cuda with faster-whisper:tiny.en
Benchmarking: The Expanse - S01E03.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  OK The Expanse - S01E03.mkv (cuda, faster-whisper:tiny.en): 10.48s avg, 0.00%
accuracy

(36/45) The Expanse - S01E04.mkv on cuda with faster-whisper:tiny.en
Benchmarking: The Expanse - S01E04.mkv (device: cuda, model: 
faster-whisper:tiny.en)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.63)
Season: 1, Episode: 1 (confidence: 0.63)
  OK The Expanse - S01E04.mkv (cuda, faster-whisper:tiny.en): 11.08s avg, 0.00%
accuracy


Testing model: faster-whisper:distil-large-v3
(37/45) Rick and Morty - S01E01.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: Rick and Morty - S01E01.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Rick and 
Morty\Rick and Morty - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  OK Rick and Morty - S01E01.mkv (cuda, faster-whisper:distil-large-v3): 7.05s 
avg, 100.00% accuracy

(38/45) South Park - s05e01.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: South Park - s05e01.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  OK South Park - s05e01.mkv (cuda, faster-whisper:distil-large-v3): 2.33s avg,
100.00% accuracy

(39/45) South Park - s05e02.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: South Park - s05e02.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  OK South Park - s05e02.mkv (cuda, faster-whisper:distil-large-v3): 2.41s avg,
0.00% accuracy

(40/45) South Park - s05e03.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: South Park - s05e03.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\South Park\South
Park - S05E01.srt (confidence: 0.79)
Season: 5, Episode: 1 (confidence: 0.79)
  OK South Park - s05e03.mkv (cuda, faster-whisper:distil-large-v3): 2.45s avg,
0.00% accuracy

(41/45) Ted Lasso - S01E01.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: Ted Lasso - S01E01.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.86)
Season: 1, Episode: 1 (confidence: 0.86)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.86)
Season: 1, Episode: 1 (confidence: 0.86)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.86)
Season: 1, Episode: 1 (confidence: 0.86)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\Ted Lasso\Ted 
Lasso - S01E01.srt (confidence: 0.86)
Season: 1, Episode: 1 (confidence: 0.86)
  OK Ted Lasso - S01E01.mkv (cuda, faster-whisper:distil-large-v3): 3.68s avg, 
100.00% accuracy

(42/45) The Expanse - S01E01.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: The Expanse - S01E01.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  OK The Expanse - S01E01.mkv (cuda, faster-whisper:distil-large-v3): 4.58s 
avg, 100.00% accuracy

(43/45) The Expanse - S01E02.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: The Expanse - S01E02.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  OK The Expanse - S01E02.mkv (cuda, faster-whisper:distil-large-v3): 4.57s 
avg, 0.00% accuracy

(44/45) The Expanse - S01E03.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: The Expanse - S01E03.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  OK The Expanse - S01E03.mkv (cuda, faster-whisper:distil-large-v3): 4.54s 
avg, 0.00% accuracy

(45/45) The Expanse - S01E04.mkv on cuda with faster-whisper:distil-large-v3
Benchmarking: The Expanse - S01E04.mkv (device: cuda, model: 
faster-whisper:distil-large-v3)
  Running iteration 1/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 2/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running iteration 3/3 on cuda...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  Running profiled test...
Matched with C:\Users\Jonathan\.mkv-episode-matcher\cache\data\The Expanse\The 
Expanse - S01E01.srt (confidence: 0.67)
Season: 1, Episode: 1 (confidence: 0.67)
  OK The Expanse - S01E04.mkv (cuda, faster-whisper:distil-large-v3): 4.56s 
avg, 0.00% accuracy

                         Performance Benchmark Results                         
+-----------------------------------------------------------------------------+
|        |        |        |        |   Avg |        |       |        |       |
|        |        |        |        |  Time |        |    F1 | Memory |       |
| File   | Model  | Device | Itera |   (s) | Accur | Score |   (MB) | Stat |
|--------+--------+--------+--------+-------+--------+-------+--------+-------|
| Rick   | whisp | cuda   |      3 |  4.79 | 100.0% | 1.000 | 1308.2 | PASS  |
| and    |        |        |        |       |        |       |        |       |
| Morty  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| South  | whisp | cuda   |      3 |  7.08 | 100.0% | 1.000 | 1315.3 | PASS  |
| Park - |        |        |        |       |        |       |        |       |
| s05e01 |        |        |        |       |        |       |        |       |
| South  | whisp | cuda   |      3 |  8.70 |   0.0% | 0.000 | 1314.4 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e02 |        |        |        |       |        |       |        |       |
| South  | whisp | cuda   |      3 |  7.95 |   0.0% | 0.000 | 1316.9 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e03 |        |        |        |       |        |       |        |       |
| Ted    | whisp | cuda   |      3 | 10.43 | 100.0% | 1.000 | 1313.2 | PASS  |
| Lasso  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | whisp | cuda   |      3 | 13.06 | 100.0% | 1.000 | 1392.6 | PASS  |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | whisp | cuda   |      3 | 12.07 |   0.0% | 0.000 | 1393.6 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E02 |        |        |        |       |        |       |        |       |
| The    | whisp | cuda   |      3 | 16.10 |  33.3% | 0.500 | 1396.5 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E03 |        |        |        |       |        |       |        |       |
| The    | whisp | cuda   |      3 | 11.75 |   0.0% | 0.000 | 1400.1 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E04 |        |        |        |       |        |       |        |       |
| Rick   | parak | cuda   |      3 | 20.07 | 100.0% | 1.000 | 2032.7 | PASS  |
| and    |        |        |        |       |        |       |        |       |
| Morty  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  1.27 | 100.0% | 1.000 | 2032.3 | PASS  |
| Park - |        |        |        |       |        |       |        |       |
| s05e01 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  1.34 |   0.0% | 0.000 | 2032.4 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e02 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  1.24 |   0.0% | 0.000 | 2033.8 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e03 |        |        |        |       |        |       |        |       |
| Ted    | parak | cuda   |      3 |  1.50 | 100.0% | 1.000 | 2034.0 | PASS  |
| Lasso  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  2.58 | 100.0% | 1.000 | 2033.1 | PASS  |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  2.07 |   0.0% | 0.000 | 2033.9 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E02 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  1.55 |   0.0% | 0.000 | 2035.9 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E03 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  1.05 |   0.0% | 0.000 | 2037.0 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E04 |        |        |        |       |        |       |        |       |
| Rick   | parak | cuda   |      3 | 30.67 | 100.0% | 1.000 | 2069.0 | PASS  |
| and    |        |        |        |       |        |       |        |       |
| Morty  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  0.97 | 100.0% | 1.000 | 2068.6 | PASS  |
| Park - |        |        |        |       |        |       |        |       |
| s05e01 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  0.95 |   0.0% | 0.000 | 2069.9 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e02 |        |        |        |       |        |       |        |       |
| South  | parak | cuda   |      3 |  1.00 |   0.0% | 0.000 | 2069.9 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e03 |        |        |        |       |        |       |        |       |
| Ted    | parak | cuda   |      3 |  1.08 | 100.0% | 1.000 | 2069.9 | PASS  |
| Lasso  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  1.74 | 100.0% | 1.000 | 2067.5 | PASS  |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  2.32 |   0.0% | 0.000 | 2066.9 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E02 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  2.38 |   0.0% | 0.000 | 2067.5 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E03 |        |        |        |       |        |       |        |       |
| The    | parak | cuda   |      3 |  2.31 |   0.0% | 0.000 | 2066.3 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E04 |        |        |        |       |        |       |        |       |
| Rick   | faste | cuda   |      3 |  5.68 | 100.0% | 1.000 | 2149.7 | PASS  |
| and    |        |        |        |       |        |       |        |       |
| Morty  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  3.07 | 100.0% | 1.000 | 2151.9 | PASS  |
| Park - |        |        |        |       |        |       |        |       |
| s05e01 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  3.19 |   0.0% | 0.000 | 2153.1 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e02 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  2.92 |   0.0% | 0.000 | 2152.0 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e03 |        |        |        |       |        |       |        |       |
| Ted    | faste | cuda   |      3 | 10.10 | 100.0% | 1.000 | 2152.5 | PASS  |
| Lasso  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 | 10.79 | 100.0% | 1.000 | 2154.5 | PASS  |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 | 10.15 |   0.0% | 0.000 | 2155.6 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E02 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 | 10.48 |   0.0% | 0.000 | 2155.7 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E03 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 | 11.08 |   0.0% | 0.000 | 2155.6 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E04 |        |        |        |       |        |       |        |       |
| Rick   | faste | cuda   |      3 |  7.05 | 100.0% | 1.000 | 2180.4 | PASS  |
| and    |        |        |        |       |        |       |        |       |
| Morty  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  2.33 | 100.0% | 1.000 | 2183.6 | PASS  |
| Park - |        |        |        |       |        |       |        |       |
| s05e01 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  2.41 |   0.0% | 0.000 | 2184.3 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e02 |        |        |        |       |        |       |        |       |
| South  | faste | cuda   |      3 |  2.45 |   0.0% | 0.000 | 2184.5 | REVI |
| Park - |        |        |        |       |        |       |        |       |
| s05e03 |        |        |        |       |        |       |        |       |
| Ted    | faste | cuda   |      3 |  3.68 | 100.0% | 1.000 | 2185.3 | PASS  |
| Lasso  |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 |  4.58 | 100.0% | 1.000 | 2186.3 | PASS  |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E01 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 |  4.57 |   0.0% | 0.000 | 2186.3 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E02 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 |  4.54 |   0.0% | 0.000 | 2186.4 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E03 |        |        |        |       |        |       |        |       |
| The    | faste | cuda   |      3 |  4.56 |   0.0% | 0.000 | 2186.4 | REVI |
| Expan |        |        |        |       |        |       |        |       |
| -      |        |        |        |       |        |       |        |       |
| S01E04 |        |        |        |       |        |       |        |       |
+-----------------------------------------------------------------------------+

Benchmark complete!
Detailed report saved to: 
D:\mkv-episode-matcher\perf-test\reports\benchmark_report_20251226_000305.json
