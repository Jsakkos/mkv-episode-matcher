Testing Parakeet on CPU...
Loading model on CPU...
[NeMo W 2025-12-25 22:49:18 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.
W1225 22:49:19.085000 36936 Lib\site-packages\torch\distributed\elastic\multiprocessing\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.
OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.
No exporters were provided. This means that no telemetry data will be collected.
[NeMo I 2025-12-25 22:49:37 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens
[NeMo W 2025-12-25 22:49:38 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
    Train config : 
    use_lhotse: true
    skip_missing_manifest_entries: true
    input_cfg: null
    tarred_audio_filepaths: null
    manifest_filepath: null
    sample_rate: 16000
    shuffle: true
    num_workers: 2
    pin_memory: true
    max_duration: 40.0
    min_duration: 0.1
    text_field: answer
    batch_duration: null
    use_bucketing: true
    bucket_duration_bins: null
    bucket_batch_size: null
    num_buckets: 30
    bucket_buffer_size: 20000
    shuffle_buffer_size: 10000
    
[NeMo W 2025-12-25 22:49:38 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). 
    Validation config : 
    use_lhotse: true
    manifest_filepath: null
    sample_rate: 16000
    batch_size: 16
    shuffle: false
    max_duration: 40.0
    min_duration: 0.1
    num_workers: 2
    pin_memory: true
    text_field: answer
    
[NeMo I 2025-12-25 22:49:38 nemo_logging:393] PADDING: 0
[NeMo I 2025-12-25 22:49:46 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-12-25 22:49:46 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo W 2025-12-25 22:49:46 nemo_logging:405] No conditional node support for Cuda.
    Cuda graphs with while loops are disabled, decoding speed will be slower
    Reason: CUDA is not available
[NeMo I 2025-12-25 22:49:46 nemo_logging:393] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo W 2025-12-25 22:49:46 nemo_logging:405] No conditional node support for Cuda.
    Cuda graphs with while loops are disabled, decoding speed will be slower
    Reason: CUDA is not available
[NeMo I 2025-12-25 22:49:49 nemo_logging:393] Model EncDecRNNTBPEModel was successfully restored from C:\Users\Jonathan\.cache\huggingface\hub\models--nvidia--parakeet-tdt-0.6b-v2\snapshots\48b630d20b000e5ad3735e5378a2d9bde3f80826\parakeet-tdt-0.6b-v2.nemo.
2025-12-25 22:49:49.051 | INFO     | mkv_episode_matcher.asr_models:load:214 - Loaded Parakeet model: nvidia/parakeet-tdt-0.6b-v2 on cpu
Loaded in 88.45s
Transcribing 
D:\mkv-episode-matcher\perf-test\debug_audio\extracted_chunk.wav...
2025-12-25 22:49:49.055 | DEBUG    | mkv_episode_matcher.asr_models:transcribe:305 - Starting Parakeet transcription for D:\mkv-episode-matcher\perf-test\debug_audio\extracted_chunk.wav
2025-12-25 22:49:56.772 | DEBUG    | mkv_episode_matcher.asr_models:_preprocess_audio:262 - Preprocessed audio saved to C:\Users\Jonathan\AppData\Local\Temp\parakeet_preprocessed\preprocessed_extracted_chunk.wav
[NeMo W 2025-12-25 22:49:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token
[NeMo W 2025-12-25 22:49:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)

Transcribing: 0it [00:00, ?it/s]
Transcribing: 1it [00:15, 15.79s/it]
Transcribing: 1it [00:15, 15.79s/it]
2025-12-25 22:50:13.259 | DEBUG    | mkv_episode_matcher.asr_models:transcribe:334 - Parakeet raw result: [Hypothesis(score=-3.1352622509002686, y_sequence=tensor([251,  27, 249, 522,  42, 516,  49, 458, 265,  39, 566, 279, 744, 130,
        132,  77, 228, 841, 121, 211, 142, 826, 307,   5,  98, 133, 826, 839,
        173, 103,  19,  31, 376, 829, 822, 103,  19, 383, 266, 290, 718, 835,
        822, 841, 248,  23, 679,   5,  24, 394,  27,  31,  23, 143, 142, 826,
         42, 740,  81, 454, 133,  89, 439,  31,  15, 341, 305, 841, 248, 596,
        142, 826,  20, 143,  31, 176, 293,  12, 102, 152,  18, 789,  31, 102,
        152, 556, 841, 123,  77, 134, 854, 163, 585, 488, 164,  41, 826, 236,
        468, 647, 610,  20,   7,   6,  87, 361,  78, 829,  30, 140, 116, 826,
        199, 260,  41, 100, 839,  31, 260,  41, 100, 236,  84,  20,   7,  41,
        841]), text='Resources that flow through our gates were never meant for us. Belters work the docks, loading and unloading precious cargo. We fix the pipes and filters that keep this rock living and breathing. We belters toil and suffer without hope and without end. And for what? One day Mars will use its might to wrest control of Ceres from Earth, and Earth will go to war.', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)], type: <class 'list'>
2025-12-25 22:50:13.260 | DEBUG    | mkv_episode_matcher.asr_models:transcribe:352 - Raw transcription: 'Resources that flow through our gates were never meant for us. Belters work the docks, loading and unloading precious cargo. We fix the pipes and filters that keep this rock living and breathing. We belters toil and suffer without hope and without end. And for what? One day Mars will use its might to wrest control of Ceres from Earth, and Earth will go to war.'
2025-12-25 22:50:13.260 | DEBUG    | mkv_episode_matcher.asr_models:transcribe:353 - Cleaned transcription: 'resources that flow through our gates were never meant for us. belters work the docks, loading and unloading precious cargo. we fix the pipes and filters that keep this rock living and breathing. we belters toil and suffer without hope and without end. and for what? one day mars will use its might to wrest control of ceres from earth, and earth will go to war.'
Transcription took 24.20s
Text: 'resources that flow through our gates were never meant for us. belters 
work the docks, loading and unloading precious cargo. we fix the pipes and 
filters that keep this rock living and breathing. we belters toil and suffer 
without hope and without end. and for what? one day mars will use its might to 
wrest control of ceres from earth, and earth will go to war.'
CPU Transcription SUCCESS
